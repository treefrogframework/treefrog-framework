.\" Man page generated from reStructuredText.
.
.TH "MONGOC_AGGREGATE" "3" "Jun 07, 2022" "1.21.2" "libmongoc"
.SH NAME
mongoc_aggregate \- Aggregation Framework Examples
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.sp
This document provides a number of practical examples that display the capabilities of the aggregation framework.
.sp
The \fI\%Aggregations using the Zip Codes Data Set\fP examples uses a publicly available data set of all zipcodes and populations in the United States. These data are available at: \fI\%zips.json\fP\&.
.SH REQUIREMENTS
.sp
Let\(aqs check if everything is installed.
.sp
Use the following command to load zips.json data set into mongod instance:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ mongoimport \-\-drop \-d test \-c zipcodes zips.json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Let\(aqs use the MongoDB shell to verify that everything was imported successfully.
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ mongo test
connecting to: test
> db.zipcodes.count()
29467
> db.zipcodes.findOne()
{
      "_id" : "35004",
      "city" : "ACMAR",
      "loc" : [
              \-86.51557,
              33.584132
      ],
      "pop" : 6055,
      "state" : "AL"
}
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AGGREGATIONS USING THE ZIP CODES DATA SET
.sp
Each document in this collection has the following form:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
  "_id" : "35004",
  "city" : "Acmar",
  "state" : "AL",
  "pop" : 6055,
  "loc" : [\-86.51557, 33.584132]
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
In these documents:
.INDENT 0.0
.IP \(bu 2
The \fB_id\fP field holds the zipcode as a string.
.IP \(bu 2
The \fBcity\fP field holds the city name.
.IP \(bu 2
The \fBstate\fP field holds the two letter state abbreviation.
.IP \(bu 2
The \fBpop\fP field holds the population.
.IP \(bu 2
The \fBloc\fP field holds the location as a \fB[latitude, longitude]\fP array.
.UNINDENT
.SH STATES WITH POPULATIONS OVER 10 MILLION
.sp
To get all states with a population greater than 10 million, use the following aggregation pipeline:
.sp
aggregation1.c
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
#include <mongoc/mongoc.h>
#include <stdio.h>

static void
print_pipeline (mongoc_collection_t *collection)
{
   mongoc_cursor_t *cursor;
   bson_error_t error;
   const bson_t *doc;
   bson_t *pipeline;
   char *str;

   pipeline = BCON_NEW ("pipeline",
                        "[",
                        "{",
                        "$group",
                        "{",
                        "_id",
                        "$state",
                        "total_pop",
                        "{",
                        "$sum",
                        "$pop",
                        "}",
                        "}",
                        "}",
                        "{",
                        "$match",
                        "{",
                        "total_pop",
                        "{",
                        "$gte",
                        BCON_INT32 (10000000),
                        "}",
                        "}",
                        "}",
                        "]");

   cursor = mongoc_collection_aggregate (
      collection, MONGOC_QUERY_NONE, pipeline, NULL, NULL);

   while (mongoc_cursor_next (cursor, &doc)) {
      str = bson_as_canonical_extended_json (doc, NULL);
      printf ("%s\en", str);
      bson_free (str);
   }

   if (mongoc_cursor_error (cursor, &error)) {
      fprintf (stderr, "Cursor Failure: %s\en", error.message);
   }

   mongoc_cursor_destroy (cursor);
   bson_destroy (pipeline);
}

int
main (int argc, char *argv[])
{
   mongoc_client_t *client;
   mongoc_collection_t *collection;
   const char *uri_string =
      "mongodb://localhost:27017/?appname=aggregation\-example";
   mongoc_uri_t *uri;
   bson_error_t error;

   mongoc_init ();

   uri = mongoc_uri_new_with_error (uri_string, &error);
   if (!uri) {
      fprintf (stderr,
               "failed to parse URI: %s\en"
               "error message:       %s\en",
               uri_string,
               error.message);
      return EXIT_FAILURE;
   }

   client = mongoc_client_new_from_uri (uri);
   if (!client) {
      return EXIT_FAILURE;
   }

   mongoc_client_set_error_api (client, 2);
   collection = mongoc_client_get_collection (client, "test", "zipcodes");

   print_pipeline (collection);

   mongoc_uri_destroy (uri);
   mongoc_collection_destroy (collection);
   mongoc_client_destroy (client);

   mongoc_cleanup ();

   return EXIT_SUCCESS;
}

.ft P
.fi
.UNINDENT
.UNINDENT
.sp
You should see a result like the following:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{ "_id" : "PA", "total_pop" : 11881643 }
{ "_id" : "OH", "total_pop" : 10847115 }
{ "_id" : "NY", "total_pop" : 17990455 }
{ "_id" : "FL", "total_pop" : 12937284 }
{ "_id" : "TX", "total_pop" : 16986510 }
{ "_id" : "IL", "total_pop" : 11430472 }
{ "_id" : "CA", "total_pop" : 29760021 }
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The above aggregation pipeline is build from two pipeline operators: \fB$group\fP and \fB$match\fP\&.
.sp
The \fB$group\fP pipeline operator requires _id field where we specify grouping; remaining fields specify how to generate composite value and must use one of the group aggregation functions: \fB$addToSet\fP, \fB$first\fP, \fB$last\fP, \fB$max\fP, \fB$min\fP, \fB$avg\fP, \fB$push\fP, \fB$sum\fP\&. The \fB$match\fP pipeline operator syntax is the same as the read operation query syntax.
.sp
The \fB$group\fP process reads all documents and for each state it creates a separate document, for example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{ "_id" : "WA", "total_pop" : 4866692 }
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBtotal_pop\fP field uses the $sum aggregation function to sum the values of all pop fields in the source documents.
.sp
Documents created by \fB$group\fP are piped to the \fB$match\fP pipeline operator. It returns the documents with the value of \fBtotal_pop\fP field greater than or equal to 10 million.
.SH AVERAGE CITY POPULATION BY STATE
.sp
To get the first three states with the greatest average population per city, use the following aggregation:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
pipeline = BCON_NEW ("pipeline", "[",
   "{", "$group", "{", "_id", "{", "state", "$state", "city", "$city", "}", "pop", "{", "$sum", "$pop", "}", "}", "}",
   "{", "$group", "{", "_id", "$_id.state", "avg_city_pop", "{", "$avg", "$pop", "}", "}", "}",
   "{", "$sort", "{", "avg_city_pop", BCON_INT32 (\-1), "}", "}",
   "{", "$limit", BCON_INT32 (3) "}",
"]");
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This aggregate pipeline produces:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{ "_id" : "DC", "avg_city_pop" : 303450.0 }
{ "_id" : "FL", "avg_city_pop" : 27942.29805615551 }
{ "_id" : "CA", "avg_city_pop" : 27735.341099720412 }
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The above aggregation pipeline is build from three pipeline operators: \fB$group\fP, \fB$sort\fP and \fB$limit\fP\&.
.sp
The first \fB$group\fP operator creates the following documents:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{ "_id" : { "state" : "WY", "city" : "Smoot" }, "pop" : 414 }
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Note, that the \fB$group\fP operator can\(aqt use nested documents except the \fB_id\fP field.
.sp
The second \fB$group\fP uses these documents to create the following documents:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{ "_id" : "FL", "avg_city_pop" : 27942.29805615551 }
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
These documents are sorted by the \fBavg_city_pop\fP field in descending order. Finally, the \fB$limit\fP pipeline operator returns the first 3 documents from the sorted set.
.SH AUTHOR
MongoDB, Inc
.SH COPYRIGHT
2017-present, MongoDB, Inc
.\" Generated by docutils manpage writer.
.
